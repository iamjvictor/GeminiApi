# rag_pipeline.py (vers√£o otimizada)

import os
import requests  # Para fazer a chamada HTTP para o seu backend Node.js
import google.generativeai as genai

# ==============================================================================
#  MODIFICA√á√ÉO 1: REMO√á√ÉO DE FUN√á√ïES DESNECESS√ÅRIAS
#  As fun√ß√µes get_text_chunks e find_most_relevant_chunks n√£o s√£o mais
#  necess√°rias AQUI. O chunking agora acontece na "f√°brica" (generateChunks.py)
#  e a busca por similaridade acontece no banco de dados.
# ==============================================================================

# A fun√ß√£o generate_embeddings ainda √© necess√°ria, mas apenas para a pergunta do usu√°rio.
def generate_embeddings(text_chunks: list[str], task_type: str) -> list[list[float]]:
    """Gera embeddings para uma lista de textos (agora usada apenas para a pergunta)."""
    print(f"Gerando embedding para 1 chunk (tarefa: {task_type})...")
    if not text_chunks:
        return []

    embedding_model = "models/text-embedding-004"
    try:
        result = genai.embed_content(
            model=embedding_model,
            content=text_chunks,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"ERRO ao gerar embedding para a pergunta: {e}")
        raise e # Lan√ßa o erro para a camada superior tratar

# ==============================================================================
#  MODIFICA√á√ÉO 2: A NOVA FUN√á√ÉO PRINCIPAL (O ORQUESTRADOR DA BUSCA)
#  Esta fun√ß√£o foi completamente reescrita. Ela n√£o processa mais o documento,
#  ela orquestra a busca r√°pida dos chunks que j√° est√£o processados no banco.
# ==============================================================================

def process_rag_pipeline(user_id: str, user_question: str) -> str:
    """
    Orquestra o processo de RAG otimizado:
    1. Gera o embedding da pergunta do usu√°rio.
    2. Busca os chunks pr√©-vetorizados mais relevantes no banco de dados (via Gateway Node.js).
    3. Retorna o contexto final para o prompt do Gemini.

    Args:
        user_id (str): ID do usu√°rio para filtrar os documentos no banco.
        user_question (str): A pergunta do usu√°rio.

    Returns:
        str: Uma string √∫nica contendo o contexto dos chunks mais relevantes.
    """
    print("üöÄ Iniciando pipeline de RAG (busca r√°pida no banco)...")
    
    # Passo 1: Gerar o embedding APENAS para a pergunta do usu√°rio.
    query_embedding_list = generate_embeddings([user_question], task_type="RETRIEVAL_QUERY")
    if not query_embedding_list:
        print("ERRO: N√£o foi poss√≠vel gerar embedding para a pergunta.")
        return ""
    query_embedding = query_embedding_list[0]

    # Passo 2: Chamar seu Gateway Node.js para que ele fa√ßa a busca vetorial no Supabase.
    print("üì° Chamando Gateway para busca de chunks por similaridade...")
    try:
        gateway_api_url = os.getenv("BACKEND_URL")
        if not gateway_api_url:
            raise ValueError("Vari√°vel de ambiente BACKEND_URL n√£o configurada.")
        
        # O payload para a requisi√ß√£o ao seu novo endpoint Node.js
        payload = {
            "user_id": user_id,
            "query_embedding": query_embedding,
            "top_k": 3 # O n√∫mero de chunks que voc√™ quer de volta
        }
        
        # Headers de autentica√ß√£o para seu middleware no Node.js
        auth_headers = {
            'x-user-id': user_id,
            'x-api-key': os.getenv("API_SECRET_KEY")
        }

        # Faz a chamada POST para o novo endpoint que voc√™ criou no Node.js
        response = requests.post(
            f"{gateway_api_url}/document-chunks/find-relevant",
            json=payload,
            headers=auth_headers
        )
        response.raise_for_status() # Lan√ßa um erro se a resposta for 4xx ou 5xx

        # A resposta do Node.js conter√° os textos dos chunks mais relevantes
        relevant_chunks = response.json().get('data', [])

    except requests.exceptions.RequestException as e:
        print(f"‚ùå ERRO ao comunicar com o Gateway Node.js: {e}")
        return "Desculpe, n√£o consegui buscar informa√ß√µes relevantes no momento."
    except Exception as e:
        print(f"‚ùå ERRO inesperado na busca de chunks: {e}")
        return "Desculpe, ocorreu um problema interno ao buscar informa√ß√µes."

    # Passo 3: Formatar o contexto final para o prompt do Gemini
    if not relevant_chunks:
        print("AVISO: Nenhuma informa√ß√£o relevante encontrada no banco de dados para esta pergunta.")
        return ""
        
    final_context = "\n\n---\n\n".join(relevant_chunks)
    print("‚úÖ Contexto de RAG (via busca no banco) finalizado e pronto para o prompt.")
    
    return final_context